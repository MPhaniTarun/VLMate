{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPP6zkHl0IGY"
      },
      "outputs": [],
      "source": [
        "def vlmate_pipeline(image_path, record_seconds=5):\n",
        "    print(\"Recording your question...\")\n",
        "    record(record_seconds)\n",
        "    print(\"Converting speech to text...\")\n",
        "    result = model.transcribe(\"recorded.wav\")\n",
        "    question = result[\"text\"]\n",
        "    print(\"You asked:\", question)\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": question}\n",
        "        ]}\n",
        "    ]\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    from transformers import TextStreamer\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "    print(\"Thinking...\")\n",
        "    response_ids = model_vlm.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    print(\"VLM Response:\", response)\n",
        "\n",
        "    tts = gTTS(text=response, lang='en')\n",
        "    tts.save(\"response.mp3\")\n",
        "\n",
        "    print(\"Speaking the answer...\")\n",
        "    return Audio(\"response.mp3\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vlmate_pipeline('/content/IMG_6812.jpeg', record_seconds=5)"
      ],
      "metadata": {
        "id": "6szPXyyf0LHl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}